Когда предсказателей несколько, данное уравнение просто расширяется для их размещения:
$$Y = b_0 + b_1X_1 + b_2X_2 + ... + b_pX_p + e$$
Вместо прямой теперь у нас линейная модель - связь между каждым коэффициентом и его переменной линейная.

**Ключевые термины для множественной линейной регрессии:**

* **Корень из среднеквадратической ошибки (root mean squared error)** -квадратной корень из среднеквадратической ошибки регрессии (это наиболее широко используемая метрика для сравнения регрессионных моделей)
* **Стандартная ошибка остатков (residual standard error)** - то же самое, что и среднеквадратическая ошибка, но скорретированная для степеней свободы.
* **R-квадрат (R-squared)** - доля дисперсии, объясняемая моделью, со значениями в интервале от 0 до 1.
* [**t-Статистика (t-statistic)**](https://github.com/sutourisu/Practical-statistic/blob/main/%D0%A1%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D1%8B%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0%20%D0%B7%D0%BD%D0%B0%D1%87%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D0%B8/%D0%9F%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B8%20%D0%BD%D0%B0%20%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%B5%20t-%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B8.md) - коэффициент для предсказателя, деленный на стандартную ошибку коэффициента, дающий метрику для сравнения важности переменных в модели.
* **Взвешенная регрессия (weighted regression)** - регрессия, в которой записям поставлены в соответствие разные веса.

### Оценивание результативности модели

Самой важной метрикой результативности с точки зрения науки о данных является *корень из среднеквадратической ошибки (RMSE)*:

$$RMSE = \sqrt{\frac{\sum\limits_{i=1}^n(y_i - \hat{y_i})^2}{n}}$$

Она измеряет совокупную точность модели и является основанием для ее сравнения с другими моделями (включая модели, подогнанные с использование специальных технические приемов машинного обучения). Похожей на RMSE является *стандартная ошибкаа остатков*. В этой случае мы имеем p предсказателей, и RSE задается следующей формулой:

$$RMSE = \sqrt{\frac{\sum\limits_{i=1}^n(y_i - \hat{y_i})^2}{n-p-1}}$$

Единственная разница состоит в том, что знаменатель является [степенями свободы](https://github.com/sutourisu/Practical-statistic/blob/main/%D0%A1%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D1%8B%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0%20%D0%B7%D0%BD%D0%B0%D1%87%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D0%B8/%D0%A1%D1%82%D0%B5%D0%BF%D0%B5%D0%BD%D0%B8%20%D1%81%D0%B2%D0%BE%D0%B1%D0%BE%D0%B4%D1%8B.md). На практике разница между RMSE и RSE для линейной регрессии являетсяочень малой, в особенности для приложений больших данных.

Еще одна полезная метрика, оторую вы увидите на выходе из вычислительных систем, называется *коэффициентом детерминации*, или $R^2$. Это значение варьирует в интервали от 0 до 1 и измеряет долю вариации в данных, объясняемую в модели. Он полезен главным образом в объяснительных применениях регрессии, где вы хотите определить, насколько хорошомодель подогнана к данных.

$$R^2 = 1 - \frac{\sum\limits_{i=1}^n(y_i - \hat{y_i})^2}{\sum\limits_{i=1}^n(y_i - \overline{y})^2}$$
### Перекрестный контроль

Перекрестный контроль расширяет идею отложенной выборки до многочисленных последовательных отложенных выборок. Алгоритм базового $k$-*блочного* *перекрестного контроля* выгляди следующим образом:

1. Отложить 1/$k$ данных в качестве отложенной выборки.
2. Натренировать модель на оставшихся данных.
3. Применить модель к отложенной выборке 1/$k$ и записать необходимые метрики оценивания результативности модели.
4. Восстановить первые 1/$k$ данных и отложить следующее 1/$k$ (исключая записи, которые были выбраны в первый раз).
5. Повторить шаги 2 и 3.
6. Повторять до тех пор, пока каждая запись не будет использована в отложенной доле.
7. Усреднить либо иным образом скомбинировать метрики анализа модели.

Деление днных на тренировочную и отложенную выборки также называют *делением на блоки*.

### Взвешенная регрессия

Взвешенная регрессия в статистике используется для разнообразных целей; в частности, она имеет большое значение для анализа сложных опросов. Исследователи данных могут посчитать взвешенную регрессию полезной в двух случаях:

* Инверсно-дисперсионное взвешивание, когда разные наблюдения были измерены с разной прецизионностью; более высоко дисперсионные получают более низкие веса;
* Анализ данных, где строки представляют многочисленные случаи; весовая переменная кодирует число исходных наблюдений, которое содержится в каждой строке.

**Ключевые идеи для множественной линейной регрессии:**

* Множественная линейная регрессия моделирует связь между переменной отклика $Y$ и многочисленными предсказательными переменными.
* Самыми важными метриками, используемыми для оценивания результативности модели, являются корень из среднеквадратической ошибки (RMSE) и коэффициент детерминации ($R^2$).
* Стандартная ошибка коэффициентов может использоваться для измерения надежности вклада переменной в модель.
* Пошаговая регрессия - это метод автоматического определения того, какие переменные должны быть включены в состав модели.
* Взвешенная регрессия используется для придания некоторым записям большего или меньшего веса при подгонке уравнения.
