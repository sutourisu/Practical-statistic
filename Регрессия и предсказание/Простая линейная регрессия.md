Простая линейная регрессия, или парная линейная регрессия, моделирует связь между величиной одной переменной и величиной второй, например по мере увеличения $X$ увеличивает и $Y$. [Корреляция](https://github.com/sutourisu/Practical-statistic/blob/main/%D0%A0%D0%B0%D0%B7%D0%B2%D0%B5%D0%B4%D1%8B%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9%20%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7/%D0%9A%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F.md) - еще один способ измерить то, каким образом две переменные связаны между собой. Разница между ними состоит в том, что корреляция измеряет силу связи между двумя переменными, тогда как регрессия оценивает природу этой связи количественно.

**Ключевые термины для простой линейной регрессии:**

* **Отклик (response)** - переменная, которую мы пытаемся предсказать.
* **Независимая переменная (independent variable)** - переменная, которая используется для предсказания отклика.
* **Запись (record)** - вектор, который состоит из значений предсказателей и значения исхода для отдельного элемента данных или случая.
* **Пересечения (intercept)** - пересечение регрессионной прямой с осью $y$, т. е. предсказанное значение, когда $X$ = 0.
* **Коэффициент регрессии (regression coefficient)** - наклон регрессионной прямой по отношению к оси $x$.
* **Подогнанные значения (fitted values)** - оценки, полученные из регрессионной прямой.
* **Остатки (residuals)** - разница между наблюдаемыми значениями и подогнанными значениями.
* **Наименьшие квадраты (least squares)** - метод подгонки регрессии путем минимизации суммы квадратов остатков.

### Уравнение регрессии

Простая линейная регрессия оценивает, насколько именно изменится $Y$, когда $X$ изменяется на некоторую величину. Для коэффициента корреляции переменные $X$ и $Y$ взаимозаменяемы. В случае регрессии мы пытаемся предсказать переменную $Y$ из переменной $X$, используя линейную связь:
$$Y = b_0 + b_1X$$
Компонент уравнения $b_0$ называется *пересечением*, а $b_1$ - *наклоном* по отношениею к оси $x$. Переменная $Y$ называется *откликом*, $X$ - *предсказателем*.

Рассмотрим диаграмму рассеяния, показывающую число лет, в течение которых рабочий был подвержен воздействию хлопчатобумажной пыли (*Exposure*) против показателя объема легких (*PEFR*). Каким образом переменная *PEFR* связана с *Exposure*? Трудно сказать что-то конкретное просто глядя на изображение.

<img src="https://i.imgur.com/Uaidw4L.png">

Простая линейная регрессия пытается отыскать "оптимальную" прямую, чтобы предсказать отклик *PEFR*, как функцию от предсказательной переменной *Exposure*.
$$PEFR = b_0 + b_1*Exposure$$
В Python мы может использовать класс **LinearRegression** из пакета **scikit-learn**.

```python
import pandas as pd
import sklearn.linear_model

data = pd.read_csv('data/LungDisease.csv')

predictors = ['Exposure']
outcome = 'PEFR'

model = sklearn.linear_model.LinearRegression()
model.fit(data[predictors], lung[outcome])

print(f'Пересечение: {model.intercept_}')
print(f'Коэффициент наклона: {model.coef_}')
```

Регрессионная прямая указанной модели:

<img src="https://i.imgur.com/0yDEtIr.png">

### Подогнанные значения и остатки

В регрессионном анализе важными понятиями являются *подогнанные значения* и *остатки*. В общем случае данные не ложатся ровно на прямую, поэтому уравнение регрессии должно включать явный остаточный член $e_i$:
$$Y = b_0 + b_1X + e_i$$
Подогнанные значения, также именуемые *предсказанными значениями*, в типичной ситуации обозначаются как $\hat{Y}_i$ ($Y$-hat). Они задаются следующей формулой:
$$\hat{Y_i} = \hat{b_0} + \hat{b_1}X_i$$ Обозначение $\hat{b_0}$ и $\hat{b_1}$ говорит о том, что эти коэффициенты оцениваются по отношению к известным, т. е. являются оценочными.

Мы вычисляем остатки $\hat{e_i}$ путем вычитания *предсказанных* значений из исходных данных:
$$\hat{e_i} = Y_i - \hat{Y_i}$$
На рисунке иллюстрируются остатки от регрессионной прямой, подогнанной к легочным данным. Остатки - это длина вертикальных пунктирный линий от данных до прямой.

<img src='https://i.imgur.com/9Rf5So9.png'>

### Наименьшие квадраты

Каким образом выполняется подгонка модели к данным? Когда существует четкя связь, вы можете мысленно представить подгонку прямой вручную. На практике прямая регрессии является оценкой, которая минимизирует значение суммы квадратичных остаткой, также именуемый *суммой квадратов остатков* или *остаточной суммой квадратов* (RSS):
$$RSS = \sum\limits_{i=1}^n(Y_i - \hat{Y_i})^2 = \sum\limits_{i=1}^n(Y_i-\hat{b_0} - \hat{b_1}X_i)^2$$
Оценки $\hat{b_0}$ и $\hat{b_1}$ - это значения, которые минимизируют сумму квадратов остатков (RSS).

Метод минимизации суммы квадратов остаткой называется *регрессией на основе меньших квадратов*.

Исторически, вычислительное удобство меньших квадратов является одной из причин широкого применения данного метода в регрессии. С появлением больших данных вычислительная скорость по-прежнему остается важным фактором. Наименьшие квадраты, как и среднее значение (см. [робастные оценки](https://github.com/sutourisu/Practical-statistic/blob/main/%D0%A0%D0%B0%D0%B7%D0%B2%D0%B5%D0%B4%D1%8B%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9%20%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7/%D0%9E%D1%86%D0%B5%D0%BD%D0%BA%D0%B8%20%D1%86%D0%B5%D0%BD%D1%82%D1%80%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE%20%D0%BF%D0%BE%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F.md)), как метод чувствителен к выбросам, хотя этот факт имеет тенденцию быть значителной проблемой только в малых по размеру выборках.

**Ключевые идеи для простой линейной регрессии:**

* Уравнение регрессии моделирует связь между переменной отклика $Y$ и предсказательной переменной $X$ в форме прямой.
* Регрессионная модель производит подогнанные значения и остатки - предсказания отклика и ошибка предсказаний.
* Подгонка регрессионных моделей в типичной ситуации выполняется методом наименьших квадратов.
* Регрессия используется как для предсказания, так и для объяснения.
